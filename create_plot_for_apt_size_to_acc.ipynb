{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mascher/anaconda3/envs/yuval_fp/lib/python3.8/site-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/mascher/anaconda3/envs/yuval_fp/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/mascher/anaconda3/envs/yuval_fp/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  4%|▍         | 10/249 [00:00<00:05, 45.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 1 out of: 249 --> 0.4016064257028112%\n",
      "number of valid scenes for evaluation: 2 out of: 249 --> 0.8032128514056224%\n",
      "number of valid scenes for evaluation: 3 out of: 249 --> 1.2048192771084338%\n",
      "number of valid scenes for evaluation: 4 out of: 249 --> 1.6064257028112447%\n",
      "number of valid scenes for evaluation: 5 out of: 249 --> 2.0080321285140563%\n",
      "number of valid scenes for evaluation: 6 out of: 249 --> 2.4096385542168677%\n",
      "number of valid scenes for evaluation: 7 out of: 249 --> 2.8112449799196786%\n",
      "number of valid scenes for evaluation: 8 out of: 249 --> 3.2128514056224895%\n",
      "number of valid scenes for evaluation: 9 out of: 249 --> 3.614457831325301%\n",
      "number of valid scenes for evaluation: 10 out of: 249 --> 4.016064257028113%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 15/249 [00:00<00:07, 32.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 11 out of: 249 --> 4.417670682730924%\n",
      "number of valid scenes for evaluation: 12 out of: 249 --> 4.819277108433735%\n",
      "number of valid scenes for evaluation: 13 out of: 249 --> 5.220883534136546%\n",
      "number of valid scenes for evaluation: 14 out of: 249 --> 5.622489959839357%\n",
      "number of valid scenes for evaluation: 15 out of: 249 --> 6.024096385542169%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 22/249 [00:00<00:05, 43.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 16 out of: 249 --> 6.425702811244979%\n",
      "number of valid scenes for evaluation: 17 out of: 249 --> 6.827309236947792%\n",
      "number of valid scenes for evaluation: 18 out of: 249 --> 7.228915662650602%\n",
      "number of valid scenes for evaluation: 19 out of: 249 --> 7.630522088353414%\n",
      "number of valid scenes for evaluation: 20 out of: 249 --> 8.032128514056225%\n",
      "number of valid scenes for evaluation: 21 out of: 249 --> 8.433734939759036%\n",
      "number of valid scenes for evaluation: 22 out of: 249 --> 8.835341365461847%\n",
      "number of valid scenes for evaluation: 23 out of: 249 --> 9.236947791164658%\n",
      "number of valid scenes for evaluation: 24 out of: 249 --> 9.63855421686747%\n",
      "number of valid scenes for evaluation: 25 out of: 249 --> 10.040160642570282%\n",
      "number of valid scenes for evaluation: 26 out of: 249 --> 10.441767068273093%\n",
      "number of valid scenes for evaluation: 27 out of: 249 --> 10.843373493975903%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 33/249 [00:00<00:04, 46.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 28 out of: 249 --> 11.244979919678714%\n",
      "number of valid scenes for evaluation: 29 out of: 249 --> 11.646586345381527%\n",
      "number of valid scenes for evaluation: 30 out of: 249 --> 12.048192771084338%\n",
      "number of valid scenes for evaluation: 31 out of: 249 --> 12.449799196787147%\n",
      "number of valid scenes for evaluation: 32 out of: 249 --> 12.851405622489958%\n",
      "number of valid scenes for evaluation: 33 out of: 249 --> 13.253012048192772%\n",
      "number of valid scenes for evaluation: 34 out of: 249 --> 13.654618473895583%\n",
      "number of valid scenes for evaluation: 35 out of: 249 --> 14.056224899598394%\n",
      "number of valid scenes for evaluation: 36 out of: 249 --> 14.457831325301203%\n",
      "number of valid scenes for evaluation: 37 out of: 249 --> 14.859437751004014%\n",
      "number of valid scenes for evaluation: 38 out of: 249 --> 15.261044176706829%\n",
      "number of valid scenes for evaluation: 39 out of: 249 --> 15.66265060240964%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 47/249 [00:00<00:03, 52.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 40 out of: 249 --> 16.06425702811245%\n",
      "number of valid scenes for evaluation: 41 out of: 249 --> 16.46586345381526%\n",
      "number of valid scenes for evaluation: 42 out of: 249 --> 16.867469879518072%\n",
      "number of valid scenes for evaluation: 43 out of: 249 --> 17.269076305220885%\n",
      "number of valid scenes for evaluation: 44 out of: 249 --> 17.670682730923694%\n",
      "number of valid scenes for evaluation: 45 out of: 249 --> 18.072289156626507%\n",
      "number of valid scenes for evaluation: 46 out of: 249 --> 18.473895582329316%\n",
      "number of valid scenes for evaluation: 47 out of: 249 --> 18.87550200803213%\n",
      "number of valid scenes for evaluation: 48 out of: 249 --> 19.27710843373494%\n",
      "number of valid scenes for evaluation: 49 out of: 249 --> 19.67871485943775%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 60/249 [00:01<00:03, 51.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 50 out of: 249 --> 20.080321285140563%\n",
      "number of valid scenes for evaluation: 51 out of: 249 --> 20.481927710843372%\n",
      "number of valid scenes for evaluation: 52 out of: 249 --> 20.883534136546185%\n",
      "number of valid scenes for evaluation: 53 out of: 249 --> 21.285140562248998%\n",
      "number of valid scenes for evaluation: 54 out of: 249 --> 21.686746987951807%\n",
      "number of valid scenes for evaluation: 55 out of: 249 --> 22.08835341365462%\n",
      "number of valid scenes for evaluation: 56 out of: 249 --> 22.48995983935743%\n",
      "number of valid scenes for evaluation: 57 out of: 249 --> 22.89156626506024%\n",
      "number of valid scenes for evaluation: 58 out of: 249 --> 23.293172690763054%\n",
      "number of valid scenes for evaluation: 59 out of: 249 --> 23.694779116465863%\n",
      "number of valid scenes for evaluation: 60 out of: 249 --> 24.096385542168676%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 66/249 [00:01<00:03, 51.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 61 out of: 249 --> 24.497991967871485%\n",
      "number of valid scenes for evaluation: 62 out of: 249 --> 24.899598393574294%\n",
      "number of valid scenes for evaluation: 63 out of: 249 --> 25.301204819277107%\n",
      "number of valid scenes for evaluation: 64 out of: 249 --> 25.702811244979916%\n",
      "number of valid scenes for evaluation: 65 out of: 249 --> 26.104417670682732%\n",
      "number of valid scenes for evaluation: 66 out of: 249 --> 26.506024096385545%\n",
      "number of valid scenes for evaluation: 67 out of: 249 --> 26.907630522088354%\n",
      "number of valid scenes for evaluation: 68 out of: 249 --> 27.309236947791167%\n",
      "number of valid scenes for evaluation: 69 out of: 249 --> 27.710843373493976%\n",
      "number of valid scenes for evaluation: 70 out of: 249 --> 28.11244979919679%\n",
      "number of valid scenes for evaluation: 71 out of: 249 --> 28.514056224899598%\n",
      "number of valid scenes for evaluation: 72 out of: 249 --> 28.915662650602407%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 79/249 [00:01<00:03, 55.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 73 out of: 249 --> 29.31726907630522%\n",
      "number of valid scenes for evaluation: 74 out of: 249 --> 29.71887550200803%\n",
      "number of valid scenes for evaluation: 75 out of: 249 --> 30.120481927710845%\n",
      "number of valid scenes for evaluation: 76 out of: 249 --> 30.522088353413658%\n",
      "number of valid scenes for evaluation: 77 out of: 249 --> 30.923694779116467%\n",
      "number of valid scenes for evaluation: 78 out of: 249 --> 31.32530120481928%\n",
      "number of valid scenes for evaluation: 79 out of: 249 --> 31.72690763052209%\n",
      "number of valid scenes for evaluation: 80 out of: 249 --> 32.1285140562249%\n",
      "number of valid scenes for evaluation: 81 out of: 249 --> 32.53012048192771%\n",
      "number of valid scenes for evaluation: 82 out of: 249 --> 32.93172690763052%\n",
      "number of valid scenes for evaluation: 83 out of: 249 --> 33.33333333333333%\n",
      "number of valid scenes for evaluation: 84 out of: 249 --> 33.734939759036145%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 91/249 [00:01<00:02, 56.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 85 out of: 249 --> 34.13654618473896%\n",
      "number of valid scenes for evaluation: 86 out of: 249 --> 34.53815261044177%\n",
      "number of valid scenes for evaluation: 87 out of: 249 --> 34.93975903614458%\n",
      "number of valid scenes for evaluation: 88 out of: 249 --> 35.34136546184739%\n",
      "number of valid scenes for evaluation: 89 out of: 249 --> 35.7429718875502%\n",
      "number of valid scenes for evaluation: 90 out of: 249 --> 36.144578313253014%\n",
      "number of valid scenes for evaluation: 91 out of: 249 --> 36.54618473895582%\n",
      "number of valid scenes for evaluation: 92 out of: 249 --> 36.94779116465863%\n",
      "number of valid scenes for evaluation: 93 out of: 249 --> 37.34939759036144%\n",
      "number of valid scenes for evaluation: 94 out of: 249 --> 37.75100401606426%\n",
      "number of valid scenes for evaluation: 95 out of: 249 --> 38.152610441767074%\n",
      "number of valid scenes for evaluation: 96 out of: 249 --> 38.55421686746988%\n",
      "number of valid scenes for evaluation: 97 out of: 249 --> 38.95582329317269%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 104/249 [00:02<00:02, 55.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 98 out of: 249 --> 39.3574297188755%\n",
      "number of valid scenes for evaluation: 99 out of: 249 --> 39.75903614457831%\n",
      "number of valid scenes for evaluation: 100 out of: 249 --> 40.16064257028113%\n",
      "number of valid scenes for evaluation: 101 out of: 249 --> 40.562248995983936%\n",
      "number of valid scenes for evaluation: 102 out of: 249 --> 40.963855421686745%\n",
      "number of valid scenes for evaluation: 103 out of: 249 --> 41.365461847389554%\n",
      "number of valid scenes for evaluation: 104 out of: 249 --> 41.76706827309237%\n",
      "number of valid scenes for evaluation: 105 out of: 249 --> 42.168674698795186%\n",
      "number of valid scenes for evaluation: 106 out of: 249 --> 42.570281124497996%\n",
      "number of valid scenes for evaluation: 107 out of: 249 --> 42.971887550200805%\n",
      "number of valid scenes for evaluation: 108 out of: 249 --> 43.373493975903614%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 116/249 [00:02<00:02, 53.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 109 out of: 249 --> 43.77510040160642%\n",
      "number of valid scenes for evaluation: 110 out of: 249 --> 44.17670682730924%\n",
      "number of valid scenes for evaluation: 111 out of: 249 --> 44.57831325301205%\n",
      "number of valid scenes for evaluation: 112 out of: 249 --> 44.97991967871486%\n",
      "number of valid scenes for evaluation: 113 out of: 249 --> 45.38152610441767%\n",
      "number of valid scenes for evaluation: 114 out of: 249 --> 45.78313253012048%\n",
      "number of valid scenes for evaluation: 115 out of: 249 --> 46.1847389558233%\n",
      "number of valid scenes for evaluation: 116 out of: 249 --> 46.58634538152611%\n",
      "number of valid scenes for evaluation: 117 out of: 249 --> 46.98795180722892%\n",
      "number of valid scenes for evaluation: 118 out of: 249 --> 47.389558232931726%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 129/249 [00:02<00:02, 56.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 119 out of: 249 --> 47.791164658634536%\n",
      "number of valid scenes for evaluation: 120 out of: 249 --> 48.19277108433735%\n",
      "number of valid scenes for evaluation: 121 out of: 249 --> 48.59437751004016%\n",
      "number of valid scenes for evaluation: 122 out of: 249 --> 48.99598393574297%\n",
      "number of valid scenes for evaluation: 123 out of: 249 --> 49.39759036144578%\n",
      "number of valid scenes for evaluation: 124 out of: 249 --> 49.79919678714859%\n",
      "number of valid scenes for evaluation: 125 out of: 249 --> 50.20080321285141%\n",
      "number of valid scenes for evaluation: 126 out of: 249 --> 50.602409638554214%\n",
      "number of valid scenes for evaluation: 127 out of: 249 --> 51.00401606425703%\n",
      "number of valid scenes for evaluation: 128 out of: 249 --> 51.40562248995983%\n",
      "number of valid scenes for evaluation: 129 out of: 249 --> 51.80722891566265%\n",
      "number of valid scenes for evaluation: 130 out of: 249 --> 52.208835341365464%\n",
      "number of valid scenes for evaluation: 131 out of: 249 --> 52.610441767068274%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 135/249 [00:02<00:02, 54.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 132 out of: 249 --> 53.01204819277109%\n",
      "number of valid scenes for evaluation: 133 out of: 249 --> 53.41365461847389%\n",
      "number of valid scenes for evaluation: 134 out of: 249 --> 53.81526104417671%\n",
      "number of valid scenes for evaluation: 135 out of: 249 --> 54.21686746987952%\n",
      "number of valid scenes for evaluation: 136 out of: 249 --> 54.61847389558233%\n",
      "number of valid scenes for evaluation: 137 out of: 249 --> 55.020080321285135%\n",
      "number of valid scenes for evaluation: 138 out of: 249 --> 55.42168674698795%\n",
      "number of valid scenes for evaluation: 139 out of: 249 --> 55.82329317269076%\n",
      "number of valid scenes for evaluation: 140 out of: 249 --> 56.22489959839358%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 147/249 [00:02<00:02, 47.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 141 out of: 249 --> 56.62650602409639%\n",
      "number of valid scenes for evaluation: 142 out of: 249 --> 57.028112449799195%\n",
      "number of valid scenes for evaluation: 143 out of: 249 --> 57.42971887550201%\n",
      "number of valid scenes for evaluation: 144 out of: 249 --> 57.831325301204814%\n",
      "number of valid scenes for evaluation: 145 out of: 249 --> 58.23293172690763%\n",
      "number of valid scenes for evaluation: 146 out of: 249 --> 58.63453815261044%\n",
      "number of valid scenes for evaluation: 147 out of: 249 --> 59.036144578313255%\n",
      "number of valid scenes for evaluation: 148 out of: 249 --> 59.43775100401606%\n",
      "number of valid scenes for evaluation: 149 out of: 249 --> 59.83935742971887%\n",
      "number of valid scenes for evaluation: 150 out of: 249 --> 60.24096385542169%\n",
      "number of valid scenes for evaluation: 151 out of: 249 --> 60.6425702811245%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 159/249 [00:03<00:01, 51.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 152 out of: 249 --> 61.044176706827315%\n",
      "number of valid scenes for evaluation: 153 out of: 249 --> 61.44578313253012%\n",
      "number of valid scenes for evaluation: 154 out of: 249 --> 61.84738955823293%\n",
      "number of valid scenes for evaluation: 155 out of: 249 --> 62.24899598393574%\n",
      "number of valid scenes for evaluation: 156 out of: 249 --> 62.65060240963856%\n",
      "number of valid scenes for evaluation: 157 out of: 249 --> 63.05220883534136%\n",
      "number of valid scenes for evaluation: 158 out of: 249 --> 63.45381526104418%\n",
      "number of valid scenes for evaluation: 159 out of: 249 --> 63.85542168674698%\n",
      "number of valid scenes for evaluation: 160 out of: 249 --> 64.2570281124498%\n",
      "number of valid scenes for evaluation: 161 out of: 249 --> 64.65863453815261%\n",
      "number of valid scenes for evaluation: 162 out of: 249 --> 65.06024096385542%\n",
      "number of valid scenes for evaluation: 163 out of: 249 --> 65.46184738955823%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 171/249 [00:03<00:01, 52.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 164 out of: 249 --> 65.86345381526104%\n",
      "number of valid scenes for evaluation: 165 out of: 249 --> 66.26506024096386%\n",
      "number of valid scenes for evaluation: 166 out of: 249 --> 66.66666666666666%\n",
      "number of valid scenes for evaluation: 167 out of: 249 --> 67.06827309236948%\n",
      "number of valid scenes for evaluation: 168 out of: 249 --> 67.46987951807229%\n",
      "number of valid scenes for evaluation: 169 out of: 249 --> 67.8714859437751%\n",
      "number of valid scenes for evaluation: 170 out of: 249 --> 68.27309236947792%\n",
      "number of valid scenes for evaluation: 171 out of: 249 --> 68.67469879518072%\n",
      "number of valid scenes for evaluation: 172 out of: 249 --> 69.07630522088354%\n",
      "number of valid scenes for evaluation: 173 out of: 249 --> 69.47791164658635%\n",
      "number of valid scenes for evaluation: 174 out of: 249 --> 69.87951807228916%\n",
      "number of valid scenes for evaluation: 175 out of: 249 --> 70.28112449799197%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 184/249 [00:03<00:01, 53.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 176 out of: 249 --> 70.68273092369478%\n",
      "number of valid scenes for evaluation: 177 out of: 249 --> 71.08433734939759%\n",
      "number of valid scenes for evaluation: 178 out of: 249 --> 71.4859437751004%\n",
      "number of valid scenes for evaluation: 179 out of: 249 --> 71.8875502008032%\n",
      "number of valid scenes for evaluation: 180 out of: 249 --> 72.28915662650603%\n",
      "number of valid scenes for evaluation: 181 out of: 249 --> 72.69076305220884%\n",
      "number of valid scenes for evaluation: 182 out of: 249 --> 73.09236947791165%\n",
      "number of valid scenes for evaluation: 183 out of: 249 --> 73.49397590361446%\n",
      "number of valid scenes for evaluation: 184 out of: 249 --> 73.89558232931726%\n",
      "number of valid scenes for evaluation: 185 out of: 249 --> 74.29718875502009%\n",
      "number of valid scenes for evaluation: 186 out of: 249 --> 74.69879518072288%\n",
      "number of valid scenes for evaluation: 187 out of: 249 --> 75.1004016064257%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 197/249 [00:03<00:00, 55.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 188 out of: 249 --> 75.50200803212851%\n",
      "number of valid scenes for evaluation: 189 out of: 249 --> 75.90361445783132%\n",
      "number of valid scenes for evaluation: 190 out of: 249 --> 76.30522088353415%\n",
      "number of valid scenes for evaluation: 191 out of: 249 --> 76.70682730923694%\n",
      "number of valid scenes for evaluation: 192 out of: 249 --> 77.10843373493977%\n",
      "number of valid scenes for evaluation: 193 out of: 249 --> 77.51004016064257%\n",
      "number of valid scenes for evaluation: 194 out of: 249 --> 77.91164658634538%\n",
      "number of valid scenes for evaluation: 195 out of: 249 --> 78.3132530120482%\n",
      "number of valid scenes for evaluation: 196 out of: 249 --> 78.714859437751%\n",
      "number of valid scenes for evaluation: 197 out of: 249 --> 79.11646586345381%\n",
      "number of valid scenes for evaluation: 198 out of: 249 --> 79.51807228915662%\n",
      "number of valid scenes for evaluation: 199 out of: 249 --> 79.91967871485943%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 209/249 [00:04<00:00, 52.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 200 out of: 249 --> 80.32128514056225%\n",
      "number of valid scenes for evaluation: 201 out of: 249 --> 80.72289156626506%\n",
      "number of valid scenes for evaluation: 202 out of: 249 --> 81.12449799196787%\n",
      "number of valid scenes for evaluation: 203 out of: 249 --> 81.52610441767068%\n",
      "number of valid scenes for evaluation: 204 out of: 249 --> 81.92771084337349%\n",
      "number of valid scenes for evaluation: 205 out of: 249 --> 82.32931726907631%\n",
      "number of valid scenes for evaluation: 206 out of: 249 --> 82.73092369477911%\n",
      "number of valid scenes for evaluation: 207 out of: 249 --> 83.13253012048193%\n",
      "number of valid scenes for evaluation: 208 out of: 249 --> 83.53413654618474%\n",
      "number of valid scenes for evaluation: 209 out of: 249 --> 83.93574297188755%\n",
      "number of valid scenes for evaluation: 210 out of: 249 --> 84.33734939759037%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 221/249 [00:04<00:00, 53.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 211 out of: 249 --> 84.73895582329317%\n",
      "number of valid scenes for evaluation: 212 out of: 249 --> 85.14056224899599%\n",
      "number of valid scenes for evaluation: 213 out of: 249 --> 85.54216867469879%\n",
      "number of valid scenes for evaluation: 214 out of: 249 --> 85.94377510040161%\n",
      "number of valid scenes for evaluation: 215 out of: 249 --> 86.34538152610442%\n",
      "number of valid scenes for evaluation: 216 out of: 249 --> 86.74698795180723%\n",
      "number of valid scenes for evaluation: 217 out of: 249 --> 87.14859437751004%\n",
      "number of valid scenes for evaluation: 218 out of: 249 --> 87.55020080321285%\n",
      "number of valid scenes for evaluation: 219 out of: 249 --> 87.95180722891565%\n",
      "number of valid scenes for evaluation: 220 out of: 249 --> 88.35341365461848%\n",
      "number of valid scenes for evaluation: 221 out of: 249 --> 88.75502008032129%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 227/249 [00:04<00:00, 48.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 222 out of: 249 --> 89.1566265060241%\n",
      "number of valid scenes for evaluation: 223 out of: 249 --> 89.5582329317269%\n",
      "number of valid scenes for evaluation: 224 out of: 249 --> 89.95983935742971%\n",
      "number of valid scenes for evaluation: 225 out of: 249 --> 90.36144578313254%\n",
      "number of valid scenes for evaluation: 226 out of: 249 --> 90.76305220883533%\n",
      "number of valid scenes for evaluation: 227 out of: 249 --> 91.16465863453816%\n",
      "number of valid scenes for evaluation: 228 out of: 249 --> 91.56626506024097%\n",
      "number of valid scenes for evaluation: 229 out of: 249 --> 91.96787148594377%\n",
      "number of valid scenes for evaluation: 230 out of: 249 --> 92.3694779116466%\n",
      "number of valid scenes for evaluation: 231 out of: 249 --> 92.7710843373494%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 238/249 [00:04<00:00, 51.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 232 out of: 249 --> 93.17269076305222%\n",
      "number of valid scenes for evaluation: 233 out of: 249 --> 93.57429718875501%\n",
      "number of valid scenes for evaluation: 234 out of: 249 --> 93.97590361445783%\n",
      "number of valid scenes for evaluation: 235 out of: 249 --> 94.37751004016064%\n",
      "number of valid scenes for evaluation: 236 out of: 249 --> 94.77911646586345%\n",
      "number of valid scenes for evaluation: 237 out of: 249 --> 95.18072289156626%\n",
      "number of valid scenes for evaluation: 238 out of: 249 --> 95.58232931726907%\n",
      "number of valid scenes for evaluation: 239 out of: 249 --> 95.98393574297188%\n",
      "number of valid scenes for evaluation: 240 out of: 249 --> 96.3855421686747%\n",
      "number of valid scenes for evaluation: 241 out of: 249 --> 96.78714859437751%\n",
      "number of valid scenes for evaluation: 242 out of: 249 --> 97.18875502008032%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:04<00:00, 51.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of valid scenes for evaluation: 243 out of: 249 --> 97.59036144578313%\n",
      "number of valid scenes for evaluation: 244 out of: 249 --> 97.99196787148594%\n",
      "number of valid scenes for evaluation: 245 out of: 249 --> 98.39357429718876%\n",
      "number of valid scenes for evaluation: 246 out of: 249 --> 98.79518072289156%\n",
      "number of valid scenes for evaluation: 247 out of: 249 --> 99.19678714859438%\n",
      "number of valid scenes for evaluation: 248 out of: 249 --> 99.59839357429718%\n",
      "number of valid scenes for evaluation: 249 out of: 249 --> 100.0%\n",
      "\n",
      "Apartment size bins (m²):\n",
      "  Bin 0: [29.07, 119.44)\n",
      "  Bin 1: [119.44, 209.82)\n",
      "  Bin 2: [209.82, 300.20)\n",
      "  Bin 3: [300.20, 390.58)\n",
      "  Bin 4: [390.58, 480.95)\n",
      "  Bin 5: [480.95, 571.33)\n",
      "  Bin 6: [571.33, 661.71)\n",
      "  Bin 7: [661.71, 752.09)\n",
      "  Bin 8: [752.09, 842.46)\n",
      "  Bin 9: [842.46, 932.84)\n",
      "No existing data found at /datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/results/final_results/visualtizations/size_vs_acc/count_dict_for_all_weights_size_only.pkl, running calculations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6368/6368 [36:04<00:00,  2.94it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import yaml\n",
    "from attrdict import AttrDict\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "from modules.mono.depth_net_pl import depth_net_pl\n",
    "from modules.semantic.semantic_net_pl import semantic_net_pl\n",
    "from data_utils.data_utils import GridSeqDataset\n",
    "from data_utils.prob_vol_data_utils import ProbVolDataset\n",
    "from utils.localization_utils import (\n",
    "    get_ray_from_depth, get_ray_from_semantics, localize, finalize_localization\n",
    ")\n",
    "from utils.data_loader_helper import load_scene_data\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Hard-coded configuration\n",
    "config = AttrDict({\n",
    "    'original_dataset_dir': '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/data/structured3d_perspective_full/Structured3D',\n",
    "    'dataset_dir': '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/data/test_data_set_full/structured3d_perspective_full',\n",
    "    'desdf_path': '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/data/test_data_set_full/desdf',\n",
    "    'log_dir_depth': '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/modules/Final_wights/depth/final_depth_model_checkpoint.ckpt',\n",
    "    'log_dir_semantic': '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/modules/Final_wights/semantic/final_semantic_model_checkpoint.ckpt',\n",
    "    'split_file': '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/data/test_data_set_full/structured3d_perspective_full/split.yaml',\n",
    "    'prob_vol_path': '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/data/test_data_set_full/prob_vols',\n",
    "    'results_dir': '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/results/full/GT',\n",
    "    'combined_prob_vols_net_path': '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/logs/combined/expected_pose_pred/combined_prob_vols_net_type-expected_pose_pred_net_size-6k_dataset_size-medium_epochs-99/combined_net-epoch=58-loss-valid=7.16.ckpt',\n",
    "    'combined_net_size': '6k',\n",
    "    'L': 0,\n",
    "    'D': 128,\n",
    "    'd_min': 0.1,\n",
    "    'd_max': 15.0,\n",
    "    'd_hyp': -0.2,\n",
    "    'F_W': 0.59587643422,\n",
    "    'V': 7,\n",
    "    'num_classes': 4,\n",
    "    'prediction_type': 'combined',\n",
    "    'use_ground_truth_depth': False,\n",
    "    'use_ground_truth_semantic': False,\n",
    "    'use_saved_prob_vol': False,\n",
    "    'num_of_scenes': -1,\n",
    "    'pad_to_max': True,\n",
    "    'max_h': 1760,\n",
    "    'max_w': 1760,\n",
    "    'weight_combinations': [\n",
    "        [1.0, 0.0],\n",
    "        [0.5, 0.5]\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "def save_data(data, path):\n",
    "    \"\"\"Saves data to a specified path using pickle.\"\"\"\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"Loads data from a specified path using pickle.\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def plot_acc_vs_count(count_dict_for_all_weights, count_name, save_path, data_type):\n",
    "    \"\"\"\n",
    "    Plots accuracy vs a given metric (e.g., binned apartment size) for different weight combinations.\n",
    "    Also includes the absolute gap between '1.0_0.0' and '0.5_0.5' accuracy as a third line.\n",
    "    \"\"\"\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Ensure required keys exist\n",
    "    if '1.0_0.0' not in count_dict_for_all_weights or '0.5_0.5' not in count_dict_for_all_weights:\n",
    "        raise ValueError(\"Both '1.0_0.0' and '0.5_0.5' keys must exist in count_dict_for_all_weights\")\n",
    "\n",
    "    # Plot mean accuracy for each weight combination\n",
    "    for weight_key in count_dict_for_all_weights.keys():\n",
    "        sorted_counts = sorted(count_dict_for_all_weights[weight_key].keys())\n",
    "        mean_acc = [np.mean(count_dict_for_all_weights[weight_key][c]) for c in sorted_counts]\n",
    "        smoothed_mean = gaussian_filter1d(mean_acc, sigma=1)  # Optionally apply smoothing\n",
    "        \n",
    "        sns.lineplot(x=sorted_counts, y=smoothed_mean, label=f'{weight_key}', linewidth=2)\n",
    "        sns.lineplot(x=sorted_counts, y=mean_acc, linewidth=1, alpha=0.4)\n",
    "    \n",
    "    # Compute the absolute gap between '1.0_0.0' and '0.5_0.5'\n",
    "    sorted_counts_gap = sorted(set(count_dict_for_all_weights['1.0_0.0'].keys()) & set(count_dict_for_all_weights['0.5_0.5'].keys()))\n",
    "    gap_mean = []\n",
    "    for count in sorted_counts_gap:\n",
    "        depth_only_acc = np.mean(count_dict_for_all_weights['1.0_0.0'][count])\n",
    "        combined_acc = np.mean(count_dict_for_all_weights['0.5_0.5'][count])\n",
    "        gap = abs(depth_only_acc - combined_acc)\n",
    "        gap_mean.append(gap)\n",
    "\n",
    "    # Plot the gap as a third line\n",
    "    if len(gap_mean) > 0:\n",
    "        sns.lineplot(x=sorted_counts_gap, y=gap_mean, label='|Depth Only - 0.5,0.5|', linewidth=2, linestyle='--', color='red')\n",
    "\n",
    "        # Add annotations for the gap values\n",
    "        for x, y in zip(sorted_counts_gap, gap_mean):\n",
    "            plt.text(x, y, f\"{y:.2f}\", color='red', fontsize=9, ha='center')\n",
    "\n",
    "    # Finalize the plot\n",
    "    plt.xlabel(f'{count_name}')\n",
    "    plt.ylabel('Mean Accuracy (m)')\n",
    "    plt.legend(title=\"Weight Combination and Gap\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_path, f'{data_type}_acc_vs_{count_name.replace(\" \", \"_\")}_with_gap.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def load_apartment_size(scene_dir):\n",
    "    \"\"\"\n",
    "    Calculate the apartment size based on the dimensions of the floorplan_semantic.png.\n",
    "    Assuming 100 pixels = 1 meter.\n",
    "    \"\"\"\n",
    "    floorplan_path = os.path.join(scene_dir, \"floorplan_semantic.png\")\n",
    "    if not os.path.exists(floorplan_path):\n",
    "        return 0.0\n",
    "\n",
    "    with Image.open(floorplan_path) as img:\n",
    "        width, height = img.size  # dimensions in pixels\n",
    "\n",
    "    # Convert to meters\n",
    "    width_m = width / 100.0\n",
    "    height_m = height / 100.0\n",
    "    area_m2 = width_m * height_m\n",
    "    return area_m2\n",
    "\n",
    "\n",
    "def evaluate_combined_model(\n",
    "    depth_net, semantic_net, desdfs, semantics, test_set, gt_poses, maps,\n",
    "    device, results_type_dir, valid_scene_names, config\n",
    "):\n",
    "    save_path = '/datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization/results/final_results/visualtizations/size_vs_acc'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    counts_file = os.path.join(save_path, 'count_dict_for_all_weights_size_only.pkl')\n",
    "    \n",
    "    # Precompute apartment sizes per scene\n",
    "    scene_to_size = {}\n",
    "    apt_sizes = []\n",
    "    for scene_name in valid_scene_names:\n",
    "        scene_dir = os.path.join(config.dataset_dir, scene_name)\n",
    "        apt_size = load_apartment_size(scene_dir)\n",
    "        scene_to_size[scene_name] = apt_size\n",
    "        apt_sizes.append(apt_size)\n",
    "\n",
    "    # Define bins for apartment sizes\n",
    "    # For example, create 10 bins between the min and max apartment size\n",
    "    if len(apt_sizes) > 0:\n",
    "        min_size = min(apt_sizes)\n",
    "        max_size = max(apt_sizes)\n",
    "        num_bins = 10 \n",
    "        bin_edges = np.linspace(min_size, max_size, num_bins+1)\n",
    "    else:\n",
    "        # Fallback if no scenes\n",
    "        bin_edges = np.array([0, 1])\n",
    "\n",
    "    # Function to get bin index for an apartment size\n",
    "    def get_bin_idx(x):\n",
    "        # digitize returns the bin index such that bin_edges[bin_idx-1] <= x < bin_edges[bin_idx]\n",
    "        # We'll use bin_idx - 1 to have 0-based indexing\n",
    "        return np.digitize(x, bin_edges) - 1\n",
    "\n",
    "    # Print the bin ranges\n",
    "    print(\"\\nApartment size bins (m²):\")\n",
    "    for i in range(len(bin_edges)-1):\n",
    "        print(f\"  Bin {i}: [{bin_edges[i]:.2f}, {bin_edges[i+1]:.2f})\")\n",
    "\n",
    "    if os.path.exists(counts_file) and False:\n",
    "        print(f\"Loading existing count data from {counts_file}\")\n",
    "        data = load_data(counts_file)\n",
    "        size_acc = data['size_acc']\n",
    "        gt_size_acc = data['gt_size_acc']\n",
    "    else:\n",
    "        print(f\"No existing data found at {counts_file}, running calculations.\")\n",
    "        wc = config.weight_combinations\n",
    "        size_acc = {f\"{dw}_{sw}\": defaultdict(list) for dw, sw in wc}\n",
    "        gt_size_acc = {f\"{dw}_{sw}\": defaultdict(list) for dw, sw in wc}\n",
    "\n",
    "        for data_idx in tqdm.tqdm(range(len(test_set))):\n",
    "            data = test_set[data_idx]\n",
    "            scene_idx = np.sum(data_idx >= np.array(test_set.scene_start_idx)) - 1\n",
    "            scene = test_set.scene_names[scene_idx]\n",
    "            if 'floor' not in scene:\n",
    "                scene = f\"scene_{int(scene.split('_')[1])}\"\n",
    "\n",
    "            if scene not in valid_scene_names:\n",
    "                continue\n",
    "\n",
    "            apartment_size = scene_to_size[scene]\n",
    "            bin_idx = get_bin_idx(apartment_size)  # Get the bin index for the apartment size\n",
    "\n",
    "            idx_within_scene = data_idx - test_set.scene_start_idx[scene_idx]\n",
    "            desdf, semantic = desdfs[scene], semantics[scene]\n",
    "            ref_pose_map = gt_poses[scene][idx_within_scene * (config.L + 1) + config.L]\n",
    "\n",
    "            # Predicted depth and semantics\n",
    "            ref_img_t = torch.tensor(data[\"ref_img\"], device=device).unsqueeze(0)\n",
    "            pred_depths = depth_net.encoder(ref_img_t, None)[0].squeeze(0).cpu().detach().numpy()\n",
    "            pred_rays_depth = get_ray_from_depth(pred_depths, V=config.V, F_W=config.F_W)\n",
    "\n",
    "            _, _, prob = semantic_net.encoder(ref_img_t, None)\n",
    "            sampled_indices_np = torch.multinomial(prob.squeeze(0), 1, replacement=True).squeeze(1).cpu().detach().numpy()\n",
    "            pred_rays_semantic = get_ray_from_semantics(sampled_indices_np, V=config.V, F_W=config.F_W)\n",
    "\n",
    "            prob_vol_pred_depth = localize(torch.tensor(desdf[\"desdf\"]), torch.tensor(pred_rays_depth), return_np=False)[0]\n",
    "            prob_vol_pred_semantic = localize(torch.tensor(semantic[\"desdf\"]), torch.tensor(pred_rays_semantic), return_np=False)[0]\n",
    "\n",
    "            # Ground truth depth and semantics\n",
    "            gt_pred_depths = data[\"ref_depth\"]\n",
    "            gt_rays_depth = get_ray_from_depth(gt_pred_depths, V=config.V, F_W=config.F_W)\n",
    "            gt_sampled_indices_np = data[\"ref_semantics\"]\n",
    "            gt_rays_semantic = get_ray_from_semantics(gt_sampled_indices_np, V=config.V, F_W=config.F_W)\n",
    "\n",
    "            gt_prob_vol_pred_depth = localize(torch.tensor(desdf[\"desdf\"]), torch.tensor(gt_rays_depth), return_np=False)[0]\n",
    "            gt_prob_vol_pred_semantic = localize(torch.tensor(semantic[\"desdf\"]), torch.tensor(gt_rays_semantic), return_np=False)[0]\n",
    "\n",
    "            # Align shapes if needed\n",
    "            def align_shapes(a, b):\n",
    "                min_shape = [min(d1, d2) for d1, d2 in zip(a.shape, b.shape)]\n",
    "                slices = tuple(slice(0, ms) for ms in min_shape)\n",
    "                return a[slices], b[slices]\n",
    "\n",
    "            d_sliced, s_sliced = align_shapes(prob_vol_pred_depth, prob_vol_pred_semantic)\n",
    "            d_gt_sliced, s_gt_sliced = align_shapes(gt_prob_vol_pred_depth, gt_prob_vol_pred_semantic)\n",
    "\n",
    "            for dw, sw in wc:\n",
    "                wkey = f\"{dw}_{sw}\"\n",
    "                combined_prob = dw * d_sliced + sw * s_sliced\n",
    "                _, pdp, _, pose_pred = finalize_localization(combined_prob)\n",
    "                pose_pred = torch.tensor(pose_pred, device=device, dtype=torch.float32)\n",
    "                pose_pred[:2] = pose_pred[:2] / 10\n",
    "                acc = torch.norm(pose_pred[:2] - torch.tensor(ref_pose_map[:2], device=device), 2).item()\n",
    "                size_acc[wkey][bin_idx].append(acc)\n",
    "\n",
    "                gt_combined_prob = dw * d_gt_sliced + sw * s_gt_sliced\n",
    "                _, gt_pdp, _, gt_pose_pred = finalize_localization(gt_combined_prob)\n",
    "                gt_pose_pred = torch.tensor(gt_pose_pred, device=device, dtype=torch.float32)\n",
    "                gt_pose_pred[:2] = gt_pose_pred[:2] / 10\n",
    "                gt_acc = torch.norm(gt_pose_pred[:2] - torch.tensor(ref_pose_map[:2], device=device), 2).item()\n",
    "                gt_size_acc[wkey][bin_idx].append(gt_acc)\n",
    "\n",
    "        data_to_save = {\n",
    "            'size_acc': size_acc,\n",
    "            'gt_size_acc': gt_size_acc,\n",
    "            'bin_edges': bin_edges\n",
    "        }\n",
    "        save_data(data_to_save, counts_file)\n",
    "\n",
    "    # Use bin indices for plotting, label them by bin midpoint for readability\n",
    "    # We'll create a new dictionary with bin midpoints as keys\n",
    "    def midpoint(a, b):\n",
    "        return (a + b) / 2.0\n",
    "\n",
    "    bin_midpoints = [midpoint(bin_edges[i], bin_edges[i+1]) for i in range(len(bin_edges)-1)]\n",
    "\n",
    "    def remap_keys(original_dict):\n",
    "        new_dict = {}\n",
    "        for wkey, bin_dict in original_dict.items():\n",
    "            new_bin_dict = {}\n",
    "            for bin_idx, values in bin_dict.items():\n",
    "                if bin_idx < len(bin_midpoints):\n",
    "                    new_bin_dict[bin_midpoints[bin_idx]] = values\n",
    "            new_dict[wkey] = new_bin_dict\n",
    "        return new_dict\n",
    "\n",
    "    size_acc_remapped = remap_keys(load_data(counts_file)['size_acc'])\n",
    "    gt_size_acc_remapped = remap_keys(load_data(counts_file)['gt_size_acc'])\n",
    "\n",
    "    plot_acc_vs_count(size_acc_remapped, 'apartment size (m²)', save_path, 'pred')\n",
    "    plot_acc_vs_count(gt_size_acc_remapped, 'apartment size (m²)', save_path, 'gt')\n",
    "\n",
    "\n",
    "def evaluate_observation(prediction_type, config, device):\n",
    "    with open(config.split_file, \"r\") as f:\n",
    "        split = AttrDict(yaml.safe_load(f))\n",
    "    scene_names = split.test[:config.num_of_scenes] if config.num_of_scenes > 0 else split.test\n",
    "\n",
    "    if config.use_saved_prob_vol:\n",
    "        test_set = ProbVolDataset(config.dataset_dir, scene_names, L=config.L, prob_vol_path=config.prob_vol_path, acc_only=False)\n",
    "    else:\n",
    "        test_set = GridSeqDataset(config.dataset_dir, scene_names, L=config.L)\n",
    "\n",
    "    depth_net, semantic_net = None, None\n",
    "    if not config.use_ground_truth_depth or not config.use_ground_truth_semantic:\n",
    "        if prediction_type in [\"depth\", \"combined\"]:\n",
    "            depth_net = depth_net_pl.load_from_checkpoint(config.log_dir_depth, d_min=config.d_min, d_max=config.d_max, d_hyp=config.d_hyp, D=config.D).to(device)\n",
    "        if prediction_type in [\"semantic\", \"combined\"]:\n",
    "            semantic_net = semantic_net_pl.load_from_checkpoint(config.log_dir_semantic, num_classes=config.num_classes).to(device)\n",
    "\n",
    "    results_type_dir = os.path.join(config.results_dir, prediction_type)\n",
    "    if config.use_ground_truth_depth or config.use_ground_truth_semantic:\n",
    "        results_type_dir = os.path.join(results_type_dir, \"gt\")\n",
    "    os.makedirs(results_type_dir, exist_ok=True)\n",
    "\n",
    "    desdfs, semantics, maps, gt_poses, valid_scene_names = load_scene_data(test_set, config.dataset_dir, config.desdf_path)\n",
    "    evaluate_combined_model(depth_net, semantic_net, desdfs, semantics, test_set, gt_poses, maps, device, results_type_dir, valid_scene_names, config)\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if config.prediction_type == \"all\":\n",
    "        for pt in [\"depth\", \"semantic\", \"combined\"]:\n",
    "            evaluate_observation(pt, config, device)\n",
    "    else:\n",
    "        evaluate_observation(config.prediction_type, config, device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
