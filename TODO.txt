Dataset preparation:
    structure3D:
        data process
        1. Generate depth_map.png & semantic_map.png
            - V-  add (0,0)
            - V- add scaling
        2. localize cameras - perspective (V)
        3. create depth & semantic raycasts for images  
            - gravity align images       
        4. create test's depth & semantic raycasts
        5. create data set:
            1. For each scene:
                1. V - create depthmap, semantic_map
                2. V - create all camera proces
                3. V - depth values
                4. V - color values
                5. V - rgb images (sorted 1...number)
                


        data preparation:
        1. V - download all perspective
        2. V - filter perspective 
        3. V - data preparation for all
                   
-------
Train: 
    1. Nets:
        - simple same net -> classification
        - simple same net -> angle to surface
        - add maskformer as final layer / masks to above Nets
    Train depth
    Train semantics
-------
Aggregate results:
     based on both


-----
Gaps:
V - no rays with distance 0 (start with 5mm)
V - fill in the walls with black -> avoid rays into walls
V - Crop images
V - enlarge doors a bit so outer walls wont have edge
V - Fix angles
V - validate rays from left to right
shape loss in semantics
V - focal view / image width? - how did they picked it?
V - Train when both are 3/5
V - create tests from "scene_03466" onward
V - 01192 - schene floor plan corropted
V - check that semantic and walls only are matching
V - Fix left to right issue
V - Fix outOfBoundry
V - Fix outdoor doors

V - Re Train depth with new maps
V - Re create desdf
V - Re evaluate

V - Investigate max dist as a Gap
V - Add whights in the loss of combine between orn and acc
V - Train with Batch size
V - change to be not 1 by 1

go over all failed to procees
---->
Bug with ~40 secenes which had no outer wall -> fixed -> might need to rerun
['scene_01044', 'scene_01264', 'scene_01554', 'scene_01589', 'scene_00159', 'scene_01858', 'scene_01892', 'scene_02048', 'scene_02091', 'scene_02202', 'scene_02286', 'scene_02427', 'scene_02575', 'scene_02600', 'scene_02776', 'scene_02785', 'scene_02907', 'scene_02959', 'scene_02971', 'scene_02975', 'scene_02977', 'scene_02982', 'scene_03137', 'scene_03145', 'scene_03296', 'scene_00482', 'scene_00483', 'scene_00559', 'scene_00576', 'scene_00640', 'scene_00668', 'scene_00776', 'scene_00833', 'scene_00924', 'scene_00978']

------
Questions:
panos or perspective only?
Full or empty?
add hitting angle to with each object
advantages of semantics VS depth in Full vs empty
Door Open/Closed - How to overcome this challenge? - maybe having category as door and window?
add roll and pitch to tests.
add staircases
add columns
Opacity handeling
mlp to connect networds
results are not as predicted (when evaluating depth on Structured 3D)
should I add Unknown semantic or leave it with walls

Zind:
fillter out outdoor imagesd

------
read: 
Resnet
cross entropy
cov Nets
Control Nets

-----
USFUL:
session managment:

tmux new -s mysession
tmux attach -t yuval_1
tmux ls
tmux switch -t session_name
------
For logs:
tensorboard --logdir=./lightning_logs/
----------


Meetings: 
09.04.24
1. Add more rays for semantic as prediction is not sufficent
2. How to localize
3. How to combine the maps
4. Control Net to combine the maps 
5. 

--
du -h --max-depth=1 /datadrive2/CRM.AI.Research/TeamFolders/Email/repo_yuval/FloorPlan/Semantic_Floor_plan_localization
--

For map and kill proccess:
ps -ef | grep python
pkill -f train_combined_model.py



====
combaine nets:
* validate all loss calculations
* validate inputs
* Read hadar paper on SD for generating maps (create guided map)
* validate training data with localization script
* how do we initilize whgits?

Nets status:
1. 2 X [H,W,O] -> [H,W,O]
2. 2 X [H,W,O] --> Max Pool (O) -> 2 X [H,W] -> [H,W]
3. 2 X [H,W,O] -> [x,y,o]
2. 2 X [H,W,O] --> Max Pool (O) -> [x,y]
3. 2 X [H,W,O] -> [x,y]

Diffusion:
- U-net -> [H,W] --> GT [H,W]
- U-net crowed diff-> [H,W] --> GT [H,W]
    - with batch size
    - with narrow GT
    - with padded and limit on size
    - with/without attention
    - 

TODO:
1. 2 X [H,W,O] -> [H,W] 
--
validate padding in evaluation
simple net for combine whgits
imageg + semantic map -> pred net

----
Combaine loss:
1. take expected return
2. loss to GT map -> then eval with max over the new net
-----
21/11
- run try diffusion of Image + semantic map --> GT
- diffrerant loss for narrow Map pred
- Investigate weight again?


30/11
why do we get much higher depth now when prob vol saved?
add all the missing test secenes

4/12 -
improve semantic by removing large door from semantic map (3250 e.g) create miss clasisfications for the semantic net
add a print summery of:
1. when combine improve depth only:
2. when combine got worse on depth only:
    - print the rays
    - print the augmented ray
- net that working with the prediction and Not with the maps....

Prompt:
I am working on solving a problem of localizing an Image taken within a house at a floorplan of the relevent floor.
I am predicting the 2d depth and semantics of that image (i,e 40 rays from left to right indicating the depth of the walls from me and the type of object(wall, door, window)
based on these prediction I create a probability map of dim (H,W) that indicates the probability of the image to be taken at x,y> resulution is 0.1m to pixcel.
So I have 2 probability volums of dim [H,W], one which is the probability based on depth and one is based on semantics.
I also have the GT probability volume of each of the map (for the case of a perfect GT prediction of the deepths and semantics).

Given these probabilty volumes I want to come up with a network to combine the 2 nets I have into a single net that I would do localization with.

.I want to desing a Diffution network that would train with the 2 maps as the conditions and would output a combined map (in the same dim) -> this output map would be used for localization.I have this paper in mind.help me with this task and network desighn.



I am working on solving a problem of localizing an Image taken within a house at a floorplan of the relevent floor.
I am predicting the 2d depth and semantics of that image (i,e 40 rays from left to right indicating the depth of the walls from me and the type of object(wall, door, window)
based on these prediction I create a probability map of dim (H,W,O) that indicates the probability of the image to be taken at x,y> resulution is 0.1m to pixcel the O is each angle posibble in jumps of 10 degs so there are 36 values for each X,Y. 
So I have 2 probability volums of dim [H,W,O], one which is the probability based on depth and one is based on semantics. 
I also have the GT probability volume of each of the map (for the case of a perfect GT prediction of the deepths and semantics).


Orentation:
normalizze -> softmax() -> get orr index ----> problem with 36* prob >> 0* prob

create [H,W] map (with orrentaion index)
softmax --> sample agian?

